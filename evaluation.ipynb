{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErinnVdSande/CGT-Project/blob/main/evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the libraries\n",
        "code taken from: https://colab.research.google.com/drive/13_jI8YLk9ATRQSd7_3rV5rOsll7jsSz0#scrollTo=xh6gb3UWjT3p"
      ],
      "metadata": {
        "id": "kZrFISpZZx82"
      },
      "id": "kZrFISpZZx82"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# ^ hides output\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb xserver-xephyr vnc4server python-opengl ffmpeg"
      ],
      "metadata": {
        "id": "x0e1beQ0XYLD"
      },
      "id": "x0e1beQ0XYLD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c57fa62",
      "metadata": {
        "id": "9c57fa62"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# ^ hides output\n",
        "!pip3 install tensorflow~=1.14.0\n",
        "\n",
        "# https://stackoverflow.com/questions/57887597/warningtensorflowentity\n",
        "!pip3 install gast==0.2.2 \n",
        "\n",
        "!pip3 install --upgrade minerl\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip3 install -U colabgymrender\n",
        "!pip3 install stable-baselines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import minerl\n",
        "from tqdm.notebook import tqdm\n",
        "from colabgymrender.recorder import Recorder\n",
        "from pyvirtualdisplay import Display\n",
        "import stable_baselines\n",
        "import numpy as np\n",
        "import random "
      ],
      "metadata": {
        "id": "_9MGyvvaa7x9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad084b70-fe84-4516-bd6d-d07e7f17360b"
      },
      "id": "_9MGyvvaa7x9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining wrappers\n",
        "Wrap MineRL with environment wrapper so that stable baselines can interface with the MineRL environment.\n",
        "\n",
        "code taken from: https://colab.research.google.com/drive/13_jI8YLk9ATRQSd7_3rV5rOsll7jsSz0#scrollTo=xh6gb3UWjT3p"
      ],
      "metadata": {
        "id": "cgVSLB2bbuLm"
      },
      "id": "cgVSLB2bbuLm"
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtractPOV(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, env,size: int = 1):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(64 * size, 64, 3))\n",
        "        self.frames = []\n",
        "        self.size = size\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # Minecraft returns shapes in NHWC by default\n",
        "        if self.size == 1:\n",
        "          return observation['pov']\n",
        "        else:\n",
        "          if not self.frames:\n",
        "            self.frames = [observation['pov'] for _ in range(self.size)]\n",
        "          else:\n",
        "            self.frames = [observation['pov']] + self.frames[:- 1]\n",
        "          return np.concatenate(self.frames)\n",
        "          \n"
      ],
      "metadata": {
        "id": "FNQIUQsgblRS"
      },
      "id": "FNQIUQsgblRS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ReversibleActionWrapper(gym.ActionWrapper):\n",
        "    \"\"\"\n",
        "    The goal of this wrapper is to add a layer of functionality on top of the normal ActionWrapper,\n",
        "    and specifically to implement a way to start:\n",
        "    (1) Construct a wrapped environment, and\n",
        "    (2) Take in actions in whatever action schema is dictated by the innermost env, and then apply all action\n",
        "    transformations/restructurings in the order they would be applied during live environment steps:\n",
        "    from the inside out\n",
        "\n",
        "    This functionality is primarily intended for converting a dataset of actions stored in the action\n",
        "    schema of the internal env into a dataset of actions stored in the schema produced by the applied set of\n",
        "    ActionWrappers, so that you can train an imitation model on such a dataset and easily transfer to the action\n",
        "    schema of the wrapped environment for rollouts, RL, etc.\n",
        "\n",
        "    Mechanically, this is done by assuming that all ActionWrappers have a `reverse_action` implemented\n",
        "    and recursively constructing a method to call all of the `reverse_action` methods from inside out.\n",
        "\n",
        "    As an example:\n",
        "        > wrapped_env = C(B(A(env)))\n",
        "    If I assume all of (A, B, and C) are action wrappers, and I pass an action to wrapped_env.step(),\n",
        "    that's equivalent to calling all of the `action` transformations from outside in:\n",
        "        > env.step(A.action(B.action(C.action(act)))\n",
        "\n",
        "    In the case covered by this wrapper, we want to perform the reverse operation, so we want to return:\n",
        "        > C.reverse_action(B.reverse_action(A.reverse_action(inner_action)))\n",
        "\n",
        "    To do this, the `wrap_action` method searches recursively for the base case where there are no more\n",
        "    `ReversibleActionWrappers` (meaning we've either reached the base env, or all of the wrappers between us and the\n",
        "    base env are not ReversibleActionWrappers) by checking whether `wrap_action` is implemented. Once we reach the base\n",
        "    case, we return self.reverse_action(inner_action), and then call all of the self.reverse_action() methods on the way\n",
        "    out of the recursion\n",
        "\n",
        "    \"\"\"\n",
        "    def wrap_action(self, inner_action):\n",
        "        \"\"\"\n",
        "        :param inner_action: An action in the format of the innermost env's action_space\n",
        "        :return: An action in the format of the action space of the fully wrapped env\n",
        "        \"\"\"\n",
        "        if hasattr(self.env, 'wrap_action'):\n",
        "            return self.reverse_action(self.env.wrap_action(inner_action))\n",
        "        else:\n",
        "            return self.reverse_action(inner_action)\n",
        "\n",
        "    def reverse_action(self, action):\n",
        "        raise NotImplementedError(\"In order to use a ReversibleActionWrapper, you need to implement a `reverse_action` function\"\n",
        "                                  \"that is the inverse of the transformation performed on an action that comes into the wrapper\")\n",
        "        \n",
        "class ActionShaping(ReversibleActionWrapper):\n",
        "  def __init__(\n",
        "            self,\n",
        "            env: gym.Env,\n",
        "            camera_angle: int = 10,\n",
        "            always_attack: bool = False,\n",
        "            camera_margin: int = 5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            env: The env to wrap.\n",
        "            camera_angle: Discretized actions will tilt the camera by this number of\n",
        "                degrees.\n",
        "            always_attack: If True, then always send attack=1 to the wrapped environment.\n",
        "            camera_margin: Used by self.wrap_action. If the continuous camera angle change\n",
        "                in a dataset action is at least `camera_margin`, then the dataset action\n",
        "                is discretized as a camera-change action.\n",
        "        \"\"\"\n",
        "        super().__init__(env)\n",
        "\n",
        "        self.camera_angle = camera_angle\n",
        "        self.camera_margin = camera_margin\n",
        "        self.always_attack = always_attack\n",
        "        self._actions = [\n",
        "            [('attack', 1)],\n",
        "            [('forward', 1)],\n",
        "            [('forward', 1), ('jump', 1)],\n",
        "            [('camera', [-self.camera_angle, 0])],\n",
        "            [('camera', [self.camera_angle, 0])],\n",
        "            [('camera', [0, self.camera_angle])],\n",
        "            [('camera', [0, -self.camera_angle])],\n",
        "        ]\n",
        "\n",
        "        self.actions = []\n",
        "        for actions in self._actions:\n",
        "            act = self.env.action_space.noop()\n",
        "            for a, v in actions:\n",
        "                act[a] = v\n",
        "            if self.always_attack:\n",
        "                act['attack'] = 1\n",
        "            self.actions.append(act)\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(len(self.actions) + 1)\n",
        "\n",
        "  def action(self, action):\n",
        "    if action == 7: \n",
        "      return self.env.action_space.noop()\n",
        "    else: \n",
        "      return self.actions[action]\n",
        "\n",
        "  def reverse_action(self, action: dict) -> np.ndarray:\n",
        "        camera_actions = action[\"camera\"].squeeze()\n",
        "        attack_actions = action[\"attack\"].squeeze()\n",
        "        forward_actions = action[\"forward\"].squeeze()\n",
        "        jump_actions = action[\"jump\"].squeeze()\n",
        "        batch_size = len(camera_actions)\n",
        "        actions = np.zeros((batch_size,), dtype=int)\n",
        "\n",
        "        for i in range(len(camera_actions)):\n",
        "            # Moving camera is most important (horizontal first)\n",
        "            if camera_actions[i][0] < -self.camera_margin:\n",
        "                actions[i] = 3\n",
        "            elif camera_actions[i][0] > self.camera_margin:\n",
        "                actions[i] = 4\n",
        "            elif camera_actions[i][1] > self.camera_margin:\n",
        "                actions[i] = 5\n",
        "            elif camera_actions[i][1] < -self.camera_margin:\n",
        "                actions[i] = 6\n",
        "            elif forward_actions[i] == 1:\n",
        "                if jump_actions[i] == 1:\n",
        "                    actions[i] = 2\n",
        "                else:\n",
        "                    actions[i] = 1\n",
        "            elif attack_actions[i] == 1:\n",
        "                actions[i] = 0\n",
        "            else:\n",
        "                # No reasonable mapping (would be no-op)\n",
        "                actions[i] = 7\n",
        "\n",
        "        return actions"
      ],
      "metadata": {
        "id": "KQwnB7fJclZA"
      },
      "id": "KQwnB7fJclZA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenserEnvironment(gym.Wrapper):\n",
        "  def __init__(self,env,good,bad):\n",
        "    super().__init__(env)\n",
        "    self.good_pixels = good\n",
        "    self.bad_pixels = bad\n",
        "\n",
        "  def step(self,action):\n",
        "    obs, og_reward, done, info = self.env.step(action)\n",
        "    reward = og_reward\n",
        "    left_top = obs[30][30]\n",
        "    right_top = obs[30][31]\n",
        "    left_bot = obs[31][30]\n",
        "    right_bot = obs[31][31]\n",
        "    for pixel in [left_top,right_top,left_bot,right_bot]:\n",
        "      if pixel in self.good_pixels:\n",
        "        reward += 0.01\n",
        "      if pixel in self.bad_pixels:\n",
        "        reward -= 0.005\n",
        "    if action == 0:\n",
        "        reward *= 2\n",
        "    return obs,reward,done,info"
      ],
      "metadata": {
        "id": "_-3Q_nrIV1ih"
      },
      "id": "_-3Q_nrIV1ih",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating a video from the policy\n",
        "code taken from: https://colab.research.google.com/drive/13_jI8YLk9ATRQSd7_3rV5rOsll7jsSz0#scrollTo=W_o_mCsADWCV"
      ],
      "metadata": {
        "id": "RN0I0wyT5_nz"
      },
      "id": "RN0I0wyT5_nz"
    },
    {
      "cell_type": "code",
      "source": [
        "def video_from_policy(env, policy, max_steps=None): \n",
        "  env = Recorder(env, './video', fps=60)\n",
        "  display = Display(visible=0, size=(800, 600))\n",
        "  display.start();\n",
        "\n",
        "  #env.seed(21)\n",
        "  obs = env.reset();\n",
        "  done = False \n",
        "  total_reward = 0\n",
        "  steps_taken = 0 \n",
        "\n",
        "  while True:\n",
        "      action, _ = policy.predict(obs)\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      total_reward += reward\n",
        "      steps_taken += 1 \n",
        "      if max_steps is not None and steps_taken > max_steps: \n",
        "        break \n",
        "      if done:\n",
        "          break\n",
        "\n",
        "  env.release()\n",
        "  env.play()"
      ],
      "metadata": {
        "id": "Ncrl0su6PNnj"
      },
      "id": "Ncrl0su6PNnj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save intermediate model in Google Drive\n",
        "code based on: https://linuxtut.com/en/4bc6d6174bb3e8461c6d/"
      ],
      "metadata": {
        "id": "xV2kSvwV69fb"
      },
      "id": "xV2kSvwV69fb"
    },
    {
      "cell_type": "code",
      "source": [
        "# connecting to Google Drive\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "MODEL_DIR = \"/content/drive/My Drive/RL-backups\"\n",
        "EVALUATION_DIR = \"/content/drive/My Drive/RL-evaluation\"\n",
        "if not os.path.exists(MODEL_DIR):  #If the directory does not exist, create it.\n",
        "    os.makedirs(MODEL_DIR)\n",
        "if not os.path.exists(EVALUATION_DIR):  #If the directory does not exist, create it.\n",
        "    os.makedirs(EVALUATION_DIR)"
      ],
      "metadata": {
        "id": "0P2YAQGG7Dlq",
        "outputId": "531bf03e-0073-4440-df02-7ae6783fdd32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0P2YAQGG7Dlq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the environment\n",
        "code taken from: https://colab.research.google.com/drive/13_jI8YLk9ATRQSd7_3rV5rOsll7jsSz0#scrollTo=W_o_mCsADWCV\n"
      ],
      "metadata": {
        "id": "wGXmELV_bXa9"
      },
      "id": "wGXmELV_bXa9"
    },
    {
      "cell_type": "code",
      "source": [
        "minerl_env = gym.make(\"MineRLTreechop-v0\")"
      ],
      "metadata": {
        "id": "pJsJVHSDbbmd"
      },
      "id": "pJsJVHSDbbmd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs_wrapped_treechop = ExtractPOV(minerl_env,4)"
      ],
      "metadata": {
        "id": "sGwCjC8qcNZM"
      },
      "id": "sGwCjC8qcNZM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs_action_wrapped_treechop = ActionShaping(obs_wrapped_treechop)"
      ],
      "metadata": {
        "id": "Rhs6SZCwdEpm"
      },
      "id": "Rhs6SZCwdEpm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines import DQN\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "O4LGFe252fQt",
        "outputId": "ffc89c30-b0f9-47ab-8199-3b361b2b8edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "O4LGFe252fQt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fdf3c8638d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "def save_to_pickle(path,name,obj):\n",
        "  if not os.path.exists(path):  #If the directory does not exist, create it.\n",
        "      os.makedirs(path)\n",
        "  with open(path+\"/\"+name+\".pkl\",\"wb\") as write_handle:\n",
        "    pickle.dump(obj,write_handle)\n",
        "\n",
        "def open_pickle(file_path):\n",
        "  with open(file_path,\"rb\") as read_handle:\n",
        "    return pickle.load(read_handle)\n"
      ],
      "metadata": {
        "id": "KtcSRA0Z2ayr"
      },
      "id": "KtcSRA0Z2ayr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(batch_path,env,max_steps=1500,steps=100,step_size=1000,test_runs=1):\n",
        "  avg_rewards = np.zeros(steps)\n",
        "  variances = np.zeros(steps)\n",
        "  rewards = np.ndarray((steps,test_runs))\n",
        "  for training_batch in range(1,steps+1):\n",
        "    # load the model\n",
        "    policy = DQN.load(MODEL_DIR + batch_path + f\"/rl_model_{training_batch * step_size}_steps\",env)\n",
        "\n",
        "    current_rewards = np.zeros(test_runs)\n",
        "\n",
        "    for run_idx in range(test_runs):  \n",
        "      # [ code from https://colab.research.google.com/drive/13_jI8YLk9ATRQSd7_3rV5rOsll7jsSz0#scrollTo=W_o_mCsADWCV\n",
        "      # reset the environment\n",
        "      obs = env.reset();\n",
        "\n",
        "      done = False \n",
        "      total_reward = 0\n",
        "      steps_taken = 0 \n",
        "\n",
        "      while True:\n",
        "          action, _ = policy.predict(obs)\n",
        "          obs, reward, done, _ = env.step(action)\n",
        "          total_reward += reward\n",
        "          steps_taken += 1 \n",
        "          if max_steps is not None and steps_taken > max_steps: \n",
        "            break \n",
        "          if done:\n",
        "              break\n",
        "      # code from https://colab.research.google.com/drive/13_jI8YLk9ATRQSd7_3rV5rOsll7jsSz0#scrollTo=W_o_mCsADWCV ]\n",
        "      current_rewards[run_idx] = total_reward\n",
        "\n",
        "    avg_rewards[training_batch-1] = np.mean(current_rewards)\n",
        "    rewards[training_batch-1] = current_rewards\n",
        "    variances[training_batch-1] = np.var(current_rewards)\n",
        "\n",
        "    current_data = dict(avg_rewards=avg_rewards[training_batch-1],\n",
        "                        variances=variances[training_batch-1],\n",
        "                        rewards=rewards[training_batch-1])\n",
        "\n",
        "    pickle_path = f\"{EVALUATION_DIR}{batch_path}\"\n",
        "    pickle_name = f\"{training_batch * step_size}\"\n",
        "    save_to_pickle(pickle_path,pickle_name,current_data)\n",
        "\n",
        "  return avg_rewards, variances, rewards"
      ],
      "metadata": {
        "id": "J2pD-QEz20QS"
      },
      "id": "J2pD-QEz20QS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_rewards, variances, rewards = evaluate_model(\"/300.000\",obs_action_wrapped_treechop)\n",
        "data_to_save = dict(avg_rewards=avg_rewards,variances=variances,rewards=rewards)\n",
        "save_to_pickle(EVALUATION_DIR,\"300_000\",data_to_save)"
      ],
      "metadata": {
        "id": "THJJA0Tm8gMm",
        "outputId": "28c03f24-0bcb-4b66-9b1a-3fcf7c2d88ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "id": "THJJA0Tm8gMm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:149: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5144b4e9a176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavg_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/300.000\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobs_action_wrapped_treechop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavg_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvariances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_to_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEVALUATION_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"300_000\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_to_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-898082ddc242>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(batch_path, env, max_steps, steps, step_size, test_runs)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m           \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0msteps_taken\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/minerl/env/_singleagent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, single_agent_action)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0maname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msingle_agent_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         }\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_agent_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/minerl/env/_multiagent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                         \u001b[0;31m# Receive the observation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_socket_recv_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         \u001b[0;31m# Receive reward done and sent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/minerl/env/malmo.py\u001b[0m in \u001b[0;36mclient_socket_recv_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclient_socket_recv_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_socket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclient_socket_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/minerl/env/comms.py\u001b[0m in \u001b[0;36mrecv_message\u001b[0;34m(sock)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecv_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mlengthbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecvall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlengthbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/minerl/env/comms.py\u001b[0m in \u001b[0;36mrecvall\u001b[0;34m(sock, count)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mnewbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnewbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MineRL-PER.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}